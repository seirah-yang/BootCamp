{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVPvglKDh+T/nrOzAJMEdl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seirah-yang/BootCamp/blob/main/post_tester(%EB%AC%B8%EC%84%9C%EC%83%9D%EC%84%B1%EA%B8%B0).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttt9ds5ZKjgv",
        "outputId": "1ae53fd6-e3bd-450a-b1bc-076aaee01772"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5pitydN-OhBR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Doc Evaluator (Refactored)\n",
        "- 문서별 Claim 자동추출 (숫자·단위 KPI, 비율, 마감/기간 등 룰 기반)\n",
        "- 하이브리드 검색: BM25 + 임베딩(BGE-M3). 임베딩/라이브러리 없으면 BM25로 폴백\n",
        "- NLI + Boolean QA 결합 판정 (임계값/우선순위 적용)\n",
        "- Major/Minor 지표 '실제' 집계 (고정값/더미 제거)\n",
        "- 한국어 문장 분할 개선, heading 기반 섹션 추출, 형식 점검 개선\n",
        "\"\"\"\n",
        "\n",
        "import os, re, json, math\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from collections import namedtuple, Counter\n",
        "import docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_EMBED_OK = False\n",
        "_BM25_OK = False\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import numpy as np\n",
        "    _EMBED_OK = True\n",
        "except Exception:\n",
        "    _EMBED_OK = False\n",
        "\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "    _BM25_OK = True\n",
        "except Exception:\n",
        "    _BM25_OK = False"
      ],
      "metadata": {
        "id": "5WQtRtxJKiOf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파서 & 전처리\n",
        "class DocParser:\n",
        "    def parse(self, docx_path: str):\n",
        "        try:\n",
        "            d = docx.Document(docx_path)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "        paras = [p.text.strip() for p in d.paragraphs if p.text and p.text.strip()]\n",
        "\n",
        "        # Heading 기반 섹션 추출\n",
        "        sections = []\n",
        "        cur_title = None\n",
        "        cur_buf = []\n",
        "        for p in d.paragraphs:\n",
        "            style = getattr(p.style, \"name\", \"\") or \"\"\n",
        "            text = p.text.strip()\n",
        "            if not text:\n",
        "                continue\n",
        "            if style.startswith(\"Heading\") or \"제목\" in style:\n",
        "                # 섹션 마감\n",
        "                if cur_title or cur_buf:\n",
        "                    sections.append({\"title\": cur_title, \"text\": \"\\n\".join(cur_buf)})\n",
        "                cur_title = text\n",
        "                cur_buf = []\n",
        "            else:\n",
        "                cur_buf.append(text)\n",
        "        if cur_title or cur_buf:\n",
        "            sections.append({\"title\": cur_title, \"text\": \"\\n\".join(cur_buf)})\n",
        "\n",
        "        full_text = \"\\n\".join(paras)\n",
        "        sentences = split_ko_sentences(full_text)\n",
        "\n",
        "        Doc = namedtuple(\"Doc\", [\"sections\", \"paragraphs\", \"sentences\", \"text\"])\n",
        "        return Doc(sections=sections, paragraphs=paras, sentences=sentences, text=full_text)"
      ],
      "metadata": {
        "id": "hS577Dw-Kvez"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_ko_sentences(text: str) -> List[str]:\n",
        "    # 문장 종결부호 + '다.' 패턴 반영\n",
        "    # 공백/줄바꿈 기준으로 재조합\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    sents = re.split(r\"(?<=[\\.?!])\\s+|(?<=다\\.)\\s+\", text)\n",
        "    sents = [s.strip() for s in sents if s and s.strip()]\n",
        "    return sents\n"
      ],
      "metadata": {
        "id": "o6YEKejrKvZa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Claim(주장) 추출\n",
        "\n",
        "KPI_PATTERNS = [\n",
        "    r\"\\b\\d{1,3}\\s?%(\\s?(감소|증가|유지))\",     # 20% 감소/증가/유지\n",
        "    r\"(오류율|에러율)\\s*\\d{1,3}\\s?%\",         # 오류율 20%\n",
        "    r\"(정확도|완전성|재현율|정밀도)\\s*(\\d{1,3}\\s?%)\",  # 정확도 95%\n",
        "    r\"(응답시간|지연)\\s*\\d+(\\.\\d+)?\\s?(ms|초)\",       # 응답시간 500ms\n",
        "    r\"(처리량|TPS|QPS)\\s*\\d+(\\.\\d+)?\",               # TPS 1000\n",
        "    r\"(기간|마감|데드라인)\\s*(\\d+\\s?(일|주|개월|월|분기|년))\",  # 기간 6개월\n",
        "    r\"(비용|원가)\\s*\\d+(,\\d{3})*(\\.\\d+)?\\s?(원|만원|억)\",        # 비용 1,000만원\n",
        "]\n",
        "\n",
        "def extract_claims(doc) -> List[Dict[str, Any]]:\n",
        "    claims = []\n",
        "    for sent in doc.sentences:\n",
        "        hit = False\n",
        "        for pat in KPI_PATTERNS:\n",
        "            if re.search(pat, sent):\n",
        "                hit = True\n",
        "                break\n",
        "        if hit:\n",
        "            claims.append({\"sent\": sent, \"type\": \"KPI\"})\n",
        "    # 너무 적으면 상위 문장(제목/요약 추정)에서도 보완 수집\n",
        "    if len(claims) < 5:\n",
        "        head_boost = doc.paragraphs[:10]\n",
        "        for s in head_boost:\n",
        "            if any(re.search(p, s) for p in KPI_PATTERNS):\n",
        "                claims.append({\"sent\": s, \"type\": \"KPI\"})\n",
        "    # 중복 제거\n",
        "    uniq = []\n",
        "    seen = set()\n",
        "    for c in claims:\n",
        "        if c[\"sent\"] not in seen:\n",
        "            uniq.append(c); seen.add(c[\"sent\"])\n",
        "    return uniq[:15] if uniq else []"
      ],
      "metadata": {
        "id": "yDcRumIoK6QV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이브리드 검색\n",
        "\n",
        "class HybridRetriever:\n",
        "    def __init__(self, cfg: Dict[str, Any], corpus_texts: List[str]):\n",
        "        self.cfg = cfg\n",
        "        self.corpus = corpus_texts or []\n",
        "        self.use_embed = False\n",
        "        self.use_bm25  = False\n",
        "\n",
        "        # BM25\n",
        "        if _BM25_OK and self.corpus:\n",
        "            tokenized = [self.tokenize(x) for x in self.corpus]\n",
        "            self.bm25 = BM25Okapi(tokenized)\n",
        "            self.use_bm25 = True\n",
        "\n",
        "        # 임베딩\n",
        "        self.embed_dim = None\n",
        "        if _EMBED_OK and self.corpus and cfg[\"models\"].get(\"embed\"):\n",
        "            try:\n",
        "                self.embed = SentenceTransformer(cfg[\"models\"][\"embed\"])\n",
        "                self.corpus_vec = self.embed.encode(self.corpus, normalize_embeddings=True)\n",
        "                self.embed_dim = self.corpus_vec.shape[1]\n",
        "                self.use_embed = True\n",
        "            except Exception:\n",
        "                self.use_embed = False\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(x: str) -> List[str]:\n",
        "        return re.findall(r\"[가-힣A-Za-z0-9]+\", x.lower())\n",
        "\n",
        "    def topk(self, query: str, k: int = 5) -> List[Tuple[str, float]]:\n",
        "        cand: Dict[int, float] = {}\n",
        "\n",
        "        # BM25 스코어\n",
        "        if self.use_bm25:\n",
        "            scores = self.bm25.get_scores(self.tokenize(query))\n",
        "            for i, s in enumerate(scores):\n",
        "                if s > 0:\n",
        "                    cand[i] = cand.get(i, 0.0) + float(s)\n",
        "\n",
        "        # 임베딩 스코어(코사인)\n",
        "        if self.use_embed:\n",
        "            qv = self.embed.encode([query], normalize_embeddings=True)[0]\n",
        "            sims = (self.corpus_vec @ qv)\n",
        "            for i, s in enumerate(sims):\n",
        "                cand[i] = cand.get(i, 0.0) + float(s) * 100.0  # 임베딩 가중(스케일 정규화 목적)\n",
        "\n",
        "        # 둘 다 실패한 경우: 부분문자열 폴백\n",
        "        if not cand:\n",
        "            for i, t in enumerate(self.corpus):\n",
        "                if query[:20] in t:\n",
        "                    cand[i] = 1.0\n",
        "\n",
        "        ranked = sorted(cand.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "        return [(self.corpus[i], score) for i, score in ranked]\n"
      ],
      "metadata": {
        "id": "uzMrFmxZK6NJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLI / QA (더미+규칙 강화)\n",
        "\n",
        "class NLIModel:\n",
        "    \"\"\"\n",
        "    외부 대형 NLI 모델이 없는 환경에서도 동작하도록 규칙 기반 점수 제공.\n",
        "    - 숫자/단위 일치도가 높으면 entailment를, 상충 수치가 발견되면 contradiction을 강화\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def predict(self, claim: str, evidence: str) -> Dict[str, float]:\n",
        "        claim_nums = extract_numbers_units(claim)\n",
        "        evid_nums  = extract_numbers_units(evidence)\n",
        "\n",
        "        # 기본 확률\n",
        "        entail = 0.33; contra = 0.33; neutr = 0.34\n",
        "        match_cnt, conflict = number_match_quality(claim_nums, evid_nums)\n",
        "\n",
        "        if match_cnt > 0 and not conflict:\n",
        "            entail = 0.7; contra = 0.1; neutr = 0.2\n",
        "        elif conflict:\n",
        "            entail = 0.1; contra = 0.7; neutr = 0.2\n",
        "        else:\n",
        "            # 유사 텍스트 힌트\n",
        "            if keyword_overlap(claim, evidence) > 0.4:\n",
        "                entail = 0.5; neutr = 0.4; contra = 0.1\n",
        "\n",
        "        max_p = max(entail, contra, neutr)\n",
        "        return {\"entail\": entail, \"contra\": contra, \"neutral\": neutr, \"max_p\": max_p}\n",
        "\n",
        "\n",
        "class BooleanQA:\n",
        "    \"\"\"아주 짧은 yes/no 룰베이스. (실모델 없을 때 폴백)\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def yesno(self, question: str, context: str) -> str:\n",
        "        # 키워드 중첩/수치 매칭이 일정 이상이면 yes\n",
        "        q = question.lower()\n",
        "        ko = keyword_overlap(q, context)\n",
        "        cn, en = extract_numbers_units(q), extract_numbers_units(context)\n",
        "        match_cnt, conflict = number_match_quality(cn, en)\n",
        "        if conflict:\n",
        "            return \"no\"\n",
        "        if match_cnt >= 1 or ko > 0.35:\n",
        "            return \"yes\"\n",
        "        return \"no\"\n"
      ],
      "metadata": {
        "id": "QaOJ4MMxK6J_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가 지표 유틸\n",
        "\n",
        "def extract_numbers_units(text: str) -> List[Tuple[float, str]]:\n",
        "    out = []\n",
        "    # 20%, 95 %, 500ms, 10개월, 1,000만원 등\n",
        "    for m in re.finditer(r\"(\\d+(?:[\\.,]\\d+)?)(\\s?%|ms|초|일|주|개월|월|분기|년|원|만원|억)?\", text):\n",
        "        num = m.group(1).replace(\",\", \"\")\n",
        "        unit = (m.group(2) or \"\").strip()\n",
        "        try:\n",
        "            out.append((float(num), unit))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return out\n",
        "\n",
        "def number_match_quality(a: List[Tuple[float,str]], b: List[Tuple[float,str]]) -> Tuple[int, bool]:\n",
        "    \"\"\"같은 단위에서 숫자가 근접하면 match, 큰 차이면 conflict로.\"\"\"\n",
        "    match = 0; conflict = False\n",
        "    for ax, au in a:\n",
        "        for bx, bu in b:\n",
        "            if au and bu and au == bu:\n",
        "                # 근사 일치(상대 오차 10% 이내)\n",
        "                if bx == 0:\n",
        "                    continue\n",
        "                rerr = abs(ax - bx) / (abs(bx) + 1e-6)\n",
        "                if rerr <= 0.1:\n",
        "                    match += 1\n",
        "                elif rerr >= 0.5:\n",
        "                    conflict = True\n",
        "    return match, conflict\n",
        "\n",
        "def keyword_overlap(a: str, b: str) -> float:\n",
        "    ta = set(re.findall(r\"[가-힣A-Za-z0-9]+\", a.lower()))\n",
        "    tb = set(re.findall(r\"[가-힣A-Za-z0-9]+\", b.lower()))\n",
        "    if not ta or not tb:\n",
        "        return 0.0\n",
        "    return len(ta & tb) / len(ta | tb)\n",
        "\n",
        "def ngram_redundancy(sentences: List[str], n: int = 3) -> float:\n",
        "    \"\"\"3-gram 중복 비율\"\"\"\n",
        "    grams = []\n",
        "    for s in sentences:\n",
        "        toks = re.findall(r\"[가-힣A-Za-z0-9]+\", s.lower())\n",
        "        grams += list(zip(*[toks[i:] for i in range(n)]))\n",
        "    if not grams:\n",
        "        return 0.0\n",
        "    c = Counter(grams)\n",
        "    dup = sum(v-1 for v in c.values() if v>1)\n",
        "    return dup / (len(grams) + 1e-6)\n",
        "\n",
        "def simple_coherence(sentences: List[str]) -> float:\n",
        "    \"\"\"아주 단순한 연결성: 인접 문장 키워드 교집합 비율 평균\"\"\"\n",
        "    if len(sentences) < 2:\n",
        "        return 0.5\n",
        "    scores = []\n",
        "    for i in range(len(sentences)-1):\n",
        "        scores.append(keyword_overlap(sentences[i], sentences[i+1]))\n",
        "    return sum(scores)/len(scores)\n",
        "\n",
        "def format_score(sections: List[Dict[str,str]], required_titles: List[str]) -> float:\n",
        "    titles = [ (s[\"title\"] or \"\").strip() for s in sections if s.get(\"title\") ]\n",
        "    hit = 0\n",
        "    for req in required_titles:\n",
        "        # 부분 일치 허용\n",
        "        if any(req in (t or \"\") for t in titles):\n",
        "            hit += 1\n",
        "    if not required_titles:\n",
        "        return 1.0\n",
        "    return hit / len(required_titles)\n",
        "\n",
        "def aggregate_scores(major: Dict[str, float], minor: Dict[str, float], glossary_on: bool) -> Dict[str, float]:\n",
        "    # terminology는 glossary 있을 때만 반영\n",
        "    term_w = 0.1 if glossary_on else 0.0\n",
        "    norm_w = 1.0 - term_w\n",
        "    # minor 합성\n",
        "    minor_core = (\n",
        "        0.4 * minor[\"coherence\"] +\n",
        "        0.3 * minor[\"fluency\"] +\n",
        "        0.2 * (1 - minor[\"redundancy\"]) +\n",
        "        0.1 * minor[\"format\"]\n",
        "    )\n",
        "    minor_total = norm_w * minor_core + term_w * minor.get(\"terminology\", 0.0)\n",
        "    final = 0.6 * major[\"accuracy\"] + 0.4 * minor_total\n",
        "    return {\"final_score\": final}"
      ],
      "metadata": {
        "id": "HxBcz080KvUg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 리포트\n",
        "def build_report(doc, results, major, minor, final, claimed_top=10) -> str:\n",
        "    rep = []\n",
        "    rep.append(\"## 문서 평가 보고서\\n\")\n",
        "    rep.append(f\"### 최종 점수: {final['final_score']:.3f}\\n\")\n",
        "    rep.append(\"### 주요 오류 분석 (Major)\\n\")\n",
        "    rep.append(f\"- Accuracy: {major['accuracy']:.3f} (entail={major['entail_cnt']}, contra={major['contra_cnt']}, unknown={major['unknown_cnt']})\\n\")\n",
        "    if \"precision\" in major:\n",
        "        rep.append(f\"- Precision: {major['precision']:.3f}, Recall: {major['recall']:.3f}, F1: {major['f1']:.3f}\\n\")\n",
        "    rep.append(\"\\n#### 개별 주장 검증 상위 결과\\n\")\n",
        "    for r in results[:claimed_top]:\n",
        "        rep.append(f\"- 주장: {r['sent']}\\n\")\n",
        "        best_txt = r['best_evidence'][:300].replace(\"\\n\", \" \")\n",
        "        rep.append(f\"  - 근거: {best_txt} ...\\n\")\n",
        "        rep.append(f\"  - 판정: {r['verdict']} (신뢰도: {r['confidence']:.2f})\\n\\n\")\n",
        "\n",
        "    rep.append(\"### 사소 오류 분석 (Minor)\\n\")\n",
        "    rep.append(f\"- Fluency: {minor['fluency']:.3f}\\n\")\n",
        "    rep.append(f\"- Coherence: {minor['coherence']:.3f}\\n\")\n",
        "    rep.append(f\"- Redundancy(낮을수록 좋음): {minor['redundancy']:.3f}\\n\")\n",
        "    rep.append(f\"- Format: {minor['format']:.3f}\\n\")\n",
        "    if \"terminology\" in minor:\n",
        "        rep.append(f\"- Terminology: {minor['terminology']:.3f}\\n\")\n",
        "\n",
        "    # 개선 제안\n",
        "    rep.append(\"\\n### 개선 제안\\n\")\n",
        "    if major[\"contra_cnt\"] > 0 or major[\"unknown_cnt\"] > 0:\n",
        "        rep.append(\"- KPI에 대한 정확한 수치/근거 인용을 본문/부록에 명시하세요(표·각주·참고문헌 라벨).\\n\")\n",
        "    if minor[\"redundancy\"] > 0.25:\n",
        "        rep.append(\"- 중복 문장을 통합하세요(요약/표로 치환, 불필요한 반복 제거).\\n\")\n",
        "    if minor[\"format\"] < 0.8:\n",
        "        rep.append(\"- 필수 섹션(연구개발 목표/내용/기대효과/추진체계/활용계획 등)의 제목을 명시하고 순서를 정리하세요.\\n\")\n",
        "    rep.append(\"- 섹션 간 연결어(따라서, 한편, 결과적으로 등)를 활용해 응집성을 높이세요.\\n\")\n",
        "\n",
        "    return \"\".join(rep)"
      ],
      "metadata": {
        "id": "ewj8LxDWLQMI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가기\n",
        "# ======================\n",
        "class DocEvaluator:\n",
        "    def __init__(self, cfg: Dict[str, Any]):\n",
        "        self.cfg = cfg\n",
        "        self.parser = DocParser()\n",
        "        self.nli = NLIModel(cfg[\"models\"].get(\"nli\"))\n",
        "        self.boolqa = BooleanQA(cfg[\"models\"].get(\"qna\"))\n",
        "        # 용어집\n",
        "        self.glossary = self._load_glossary(cfg[\"inputs\"].get(\"domain_glossary\"))\n",
        "\n",
        "    def _load_glossary(self, file_path: str):\n",
        "        if not file_path or not os.path.exists(file_path):\n",
        "            return None\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return json.load(f)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def evaluate(self, docx_path: str) -> Dict[str, Any]:\n",
        "        doc = self.parser.parse(docx_path)\n",
        "        if not doc:\n",
        "            return {\"final_score\": 0.0, \"error\": \"Document parsing failed\"}\n",
        "\n",
        "        # 코퍼스(근거 후보): 문장 + 섹션 텍스트\n",
        "        corpus = doc.sentences[:]\n",
        "        for s in doc.sections:\n",
        "            corpus.append(s[\"text\"])\n",
        "        retriever = HybridRetriever(self.cfg, corpus)\n",
        "\n",
        "        # 1) Claim 추출\n",
        "        claims = extract_claims(doc)\n",
        "        if not claims:\n",
        "            # 주장 없으면 문서 핵심문장(상위 10개)로 대체\n",
        "            base = [{\"sent\": s, \"type\": \"fallback\"} for s in doc.sentences[:10]]\n",
        "            claims = base\n",
        "\n",
        "        # 2) 검증\n",
        "        results = []\n",
        "        entail_cnt = contra_cnt = unknown_cnt = 0\n",
        "        for c in claims:\n",
        "            top = retriever.topk(c[\"sent\"], k=5)\n",
        "            evidences = [t for t, sc in top]\n",
        "            best = evidences[0] if evidences else \"\"\n",
        "            nli = self.nli.predict(c[\"sent\"], best)\n",
        "            verdict, conf = self._post_decide(c[\"sent\"], best, nli)\n",
        "\n",
        "            if verdict == \"entailment\": entail_cnt += 1\n",
        "            elif verdict == \"contradiction\": contra_cnt += 1\n",
        "            else: unknown_cnt += 1\n",
        "\n",
        "            results.append({\n",
        "                **c,\n",
        "                \"best_evidence\": best,\n",
        "                \"verdict\": verdict,\n",
        "                \"confidence\": conf\n",
        "            })\n",
        "\n",
        "             # 3) Major 지표\n",
        "        tot = max(1, (entail_cnt + contra_cnt + unknown_cnt))\n",
        "        accuracy = entail_cnt / tot\n",
        "        # QA 기반 precision/recall 추정(간단 정의): yes=정답, no=오답 가정\n",
        "        tp = entail_cnt\n",
        "        fp = contra_cnt\n",
        "        fn = unknown_cnt\n",
        "        precision = tp / (tp + fp + 1e-6)\n",
        "        recall    = tp / (tp + fn + 1e-6)\n",
        "        f1        = 2*precision*recall / (precision+recall+1e-6)\n",
        "        major_scores = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"entail_cnt\": entail_cnt,\n",
        "            \"contra_cnt\": contra_cnt,\n",
        "            \"unknown_cnt\": unknown_cnt\n",
        "        }\n",
        " # 3) Major 지표\n",
        "        tot = max(1, (entail_cnt + contra_cnt + unknown_cnt))\n",
        "        accuracy = entail_cnt / tot\n",
        "        # QA 기반 precision/recall 추정(간단 정의): yes=정답, no=오답 가정\n",
        "        tp = entail_cnt\n",
        "        fp = contra_cnt\n",
        "        fn = unknown_cnt\n",
        "        precision = tp / (tp + fp + 1e-6)\n",
        "        recall    = tp / (tp + fn + 1e-6)\n",
        "        f1        = 2*precision*recall / (precision+recall+1e-6)\n",
        "        major_scores = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"entail_cnt\": entail_cnt,\n",
        "            \"contra_cnt\": contra_cnt,\n",
        "            \"unknown_cnt\": unknown_cnt\n",
        "        }\n",
        "    def _post_decide(self, clain: str, evidence: str, nli_out: Dict[str, float]) -> Tuple[str, float]:\n",
        "         # 임계값\n",
        "         conf = nli_out[\"max_p\"]\n",
        "         if conf >= 0.65:\n",
        "            return self.argmax_verdic(nli_out), conf\n",
        "\n",
        "            qa_ans = self.boolqa.yesno(f\"Is the claim supported? {claim}\", evidence)\n",
        "            if qa_ans.lower().startswith(\"y\"):\n",
        "                return \"entailment\", max(0.65, conf)\n",
        "            else:\n",
        "                return \"contradiction\", max(0.65, conf)\n",
        "\n",
        "    @staticmethod\n",
        "    def _argmax_verdict(nli_out: Dict[str,float]) -> str:\n",
        "        m = max(((\"entailment\", nli_out[\"entail\"]),\n",
        "                (\"contradiction\", nli_out[\"contra\"]),\n",
        "                (\"neutral\", nli_out[\"neutral\"])), key=lambda x:x[1])[0]\n",
        "        return m\n",
        "\n",
        "    def _fluency(self, sents: List[str]) -> float:\n",
        "        # 간이 유창성: 평균 문장 길이·기호 비율을 바탕으로 0~1 스코어\n",
        "        if not sents:\n",
        "            return 0.5\n",
        "\n",
        "        lens = [len(s) for s in sents]\n",
        "        mean_len = sum(lens)/len(lens)\n",
        "        punct = sum(ch in \".,;:?!~\" for s in sents for ch in s) / (sum(lens)+1e-6)\n",
        "\n",
        "        # 경험적 매핑\n",
        "        score = 0.5 + 0.5 * math.tanh((mean_len-25)/50) - 0.2*abs(punct-0.03)\n",
        "        return max(0.0, min(1.0, score))\n",
        "\n",
        "        def _terminology_score(self, text: str):\n",
        "            if not self.glossary:\n",
        "                return None\n",
        "            terms = set(self.glossary.get(\"terms\", []))\n",
        "            if not terms:\n",
        "                return None\n",
        "            toks = set(re.findall(r\"[가-힣A-Za-z0-9\\-]+\", text))\n",
        "            hit = len(terms & toks)\n",
        "            return hit / len(terms)\n"
      ],
      "metadata": {
        "id": "cNE_AB4sLlTM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    config = {\n",
        "        \"models\": {\n",
        "            \"embed\": \"BAAI/bge-m3\",          # 있으면 사용, 없으면 자동 폴백\n",
        "            \"nli\":  \"rule-lite\",\n",
        "            \"qna\":  \"rule-lite\"\n",
        "        },\n",
        "        \"inputs\": {\n",
        "            \"domain_glossary\": None\n",
        "        },\n",
        "        \"format_required\": [\n",
        "            \"연구개발 목표\", \"연구개발 내용\", \"연구개발성과 활용계획\", \"추진체계\", \"기대효과\"\n",
        "        ],\n",
        "        \"output_dir\": \"./eval_reports\"\n",
        "    }"
      ],
      "metadata": {
        "id": "oZrv1UczKiLC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import argparse\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--doc\", required=True, help=\"/content/e5_연구계발계획서v6.docx\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    ev = DocEvaluator(config)\n",
        "    out = ev.evaluate(args.doc)\n",
        "    print(f\"[DONE] score={out['final_score']:.3f}, report={out['report_path']}\")"
      ],
      "metadata": {
        "id": "VkmyKW_PKiH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import argparse\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--doc\", required=True, help=\"/content/cde_연구계발계획서.docx\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    ev = DocEvaluator(config)\n",
        "    out = ev.evaluate(args.doc)\n",
        "    print(f\"[DONE] score={out['final_score']:.3f}, report={out['report_path']}\")"
      ],
      "metadata": {
        "id": "ZzCs7ItqN4Fp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}